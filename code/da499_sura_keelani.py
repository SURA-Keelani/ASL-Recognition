# -*- coding: utf-8 -*-
"""DA499_Sura_Keelani

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/surakeelani/da499-sura-keelani.fba771fd-8287-4562-8c1c-234f072a50ea.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250803/auto/storage/goog4_request%26X-Goog-Date%3D20250803T142225Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D72f3092402f5884a31fe133689f196745853748d9e5f8302cd3dc58091612a383b49d21f9350e8eba2986fa9a958838b46fd1aa7e51633f35ecc29bcad0efcc1022bca56abffa540b7d32ad18d81c639c337aca1f7a6cdfda17140932d017f0a909183cd1cdb9b544a6d322b7657d1a563e77a156a9b873d74fee51c6a2fb50217b459b67d8da144677a30a3543f4a7e7f4ac9e1dc312afa351ec4ccc4c9ba1977f1ed4f59164f517fba60233ca3ec1a17492c9cae3a82c81235a57afec9d58ad763a275f3928d82006bb20e5dd69d266f27c0ccb91679f1f5627eac542fb779a1b78daa22a26794aff95d563a5932ac1a44be4d6661ce3794e315ca2b6ba099
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
grassknoted_asl_alphabet_path = kagglehub.dataset_download('grassknoted/asl-alphabet')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import kagglehub

# Download latest version
path = kagglehub.dataset_download("grassknoted/asl-alphabet")

print("Path to dataset files:", path)

import numpy as np  # Library for numerical computations
import pandas as pd  # Library for handling and analyzing structured data

import seaborn as sns  # Library for creating statistical data visualizations
import matplotlib.pyplot as plt  # Library for creating plots and graphs


import cv2  # Library for computer vision tasks such as image manipulation
import skimage  # Library for image processing and analysis
from skimage.transform import resize  # Specific method for resizing images from the skimage library

import tensorflow as tf  # TensorFlow for machine learning and deep learning tasks
from tensorflow import keras  # High-level API within TensorFlow for building and training deep learning models

# Importing a library to interact with the operating system
import os  # Module to handle file system paths and operations


print("Done!")

print(os.listdir('/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'))

# Define constants for data preprocessing and model training
batch_size = 64  # Number of samples per batch during training
imageSize = 64  # Desired size (width and height) for resizing images
target_dims = (imageSize, imageSize, 3)  # Target dimensions for input images
num_classes = 29  # Number of classes in the dataset

# Training data length and directory path
train_len = 87000  # Total number of training images
train_dir = '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'  # Path to the dataset folder

# Function to load and preprocess the dataset
def get_data(folder):
    """
    Load images and labels from the specified folder.

    Args:
        folder (str): Path to the dataset folder.

    Returns:
        tuple: Preprocessed images (X) and their corresponding labels (y).
    """
    # Initialize arrays to store images and labels
    X = np.empty((train_len, imageSize, imageSize, 3), dtype=np.float32)
    y = np.empty((train_len,), dtype=int)
    cnt = 0  # Counter for tracking the index of the current image

    # Define a dictionary mapping folder names to label indices
    label_mapping = {
        'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9,
        'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19,
        'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28
    }

    # Iterate through each folder in the specified directory
    for folderName in os.listdir(folder):
        # Ignore hidden files or directories
        if not folderName.startswith('.'):
            # Get the label index from the mapping dictionary
            label = label_mapping.get(folderName, 29)  # Default to 29 for unknown labels

            # Construct the full path to the subdirectory
            folder_path = os.path.join(folder, folderName)

            # Iterate through each image file in the subdirectory
            for image_filename in os.listdir(folder_path):
                img_path = os.path.join(folder_path, image_filename)

                # Read the image using OpenCV
                img_file = cv2.imread(img_path)
                if img_file is not None:  # Ensure the image is valid
                    # Resize the image to the target dimensions
                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))

                    # Convert the image to a NumPy array and reshape it
                    img_arr = np.asarray(img_file, dtype=np.float32)

                    # Assign the image and label to the dataset arrays
                    X[cnt] = img_arr
                    y[cnt] = label
                    cnt += 1

    return X, y  # Return the preprocessed images and labels

# Load the training data
X_train, y_train = get_data(train_dir)

# Print a success message
print(f"Images successfully imported. Total images processed: {len(y_train)}")

print("The shape of X_train is : ", X_train.shape)
print("The shape of y_train is : ", y_train.shape)

print("The shape of one image is : ", X_train[0].shape)

plt.imshow(X_train[80])
plt.show()

X_data = X_train.copy()
y_data = y_train.copy()

from sklearn.model_selection import train_test_split

# Split into training (70%) and testing (30%)
X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.3, random_state=42)

# Further split the testing data into validation (15%) and testing (15%)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"Training set: {X_train.shape}, Validation set: {X_val.shape}, Testing set: {X_test.shape}")

from tensorflow.keras.utils import to_categorical

# Dynamically determine the number of unique classes
num_classes = len(np.unique(y_train))

# One-hot encode the labels for training, validation, and testing
y_cat_train = to_categorical(y_train, num_classes)
y_cat_val = to_categorical(y_val, num_classes)
y_cat_test = to_categorical(y_test, num_classes)

# Print shapes to confirm
print(f"Training labels shape: {y_cat_train.shape}")
print(f"Validation labels shape: {y_cat_val.shape}")
print(f"Testing labels shape: {y_cat_test.shape}")

# Checking the dimensions of all the variables
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(X_val.shape)
print(y_val.shape)
print(y_cat_train.shape)
print(y_cat_test.shape)
print(y_cat_val.shape)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.applications import VGG16, VGG19, ResNet50, MobileNet

# Define the CNN model
model = Sequential([
    # First convolutional layer with ReLU activation
    Conv2D(32, (3, 3), input_shape=(64, 64, 3)),  # 32 filters, kernel size 3x3, input size 64x64x3 (RGB image)
    BatchNormalization(), # Normalize inputs for faster convergence
    Activation('relu'),  # Apply ReLU activation

    # First pooling layer
    MaxPooling2D(pool_size=(2, 2)),  # Downsample feature maps by 2x2

    # Second convolutional layer with ReLU activation
    Conv2D(64, (3, 3)),  # 64 filters, kernel size 3x3
    Activation('relu'),

    # Second pooling layer
    MaxPooling2D(pool_size=(2, 2)),

    # Third convolutional layer with ReLU activation
    Conv2D(64, (3, 3)),  # 64 filters, kernel size 3x3
    Activation('relu'),

    # Third pooling layer
    MaxPooling2D(pool_size=(2, 2)),

    # Flatten layer to convert 3D feature maps to 1D
    Flatten(),

    # Fully connected dense layer with 128 neurons
    Dense(128),
    Activation('relu'), # ReLU activation for the dense layer

    Dropout(0.5),  # Dropout layer to prevent overfitting

    # Output layer with 29 neurons (number of classes) and softmax activation
    Dense(29),
    Activation('softmax')  # Softmax activation for multi-class classification
])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)

model.compile(
    optimizer='adam',  # Use Adam optimizer for efficient training
    loss='categorical_crossentropy',  # Appropriate loss for multi-class classification
    metrics=['accuracy']  # Track accuracy during training
)

history = model.fit(
    X_train, y_cat_train,  # Training data and labels
    epochs=15,  # Maximum number of epochs
    batch_size=64,  # Batch size for training
    validation_data=(X_test, y_cat_test),  # Validation data for evaluation after each epoch
)

# Convert model training history to a pandas DataFrame
metrics = pd.DataFrame(model.history.history)

# Print the model metrics
print("The model metrics are")
print(metrics)

import matplotlib.pyplot as plt

# Plotting training and validation accuracy
plt.plot(metrics['accuracy'], label='Train Accuracy')
plt.plot(metrics['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and validation loss
plt.plot(metrics['loss'], label='Train Loss')
plt.plot(metrics['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_cat_test, verbose=0)

# Output the test loss and accuracy
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

# Load the pre-trained VGG16 model without the top classification layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# Add custom classification layers
model_vgg16 = Sequential([
    base_model,  # Use the pre-trained base
    Flatten(),  # Flatten the feature maps
    Dense(256, activation='relu'),  # Fully connected layer
    Dropout(0.5),  # Dropout for regularization
    Dense(29, activation='softmax')  # Output layer for 29 classes
])

# Freeze the base model layers to prevent training them
base_model.trainable = False

model_vgg16.summary()

model_vgg16.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_vgg16 = model_vgg16.fit(
    X_train, y_cat_train,
    epochs=15,
    batch_size=64,
    validation_data=(X_test, y_cat_test),
    callbacks=[early_stop],
    verbose=2
)

metrics_vgg16 = pd.DataFrame(model_vgg16.history.history)
print("The model metrics are")
print(metrics_vgg16)

import matplotlib.pyplot as plt

# Plotting training and validation accuracy
plt.plot(metrics_vgg16['accuracy'], label='Train Accuracy')
plt.plot(metrics_vgg16['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and validation loss
plt.plot(metrics_vgg16['loss'], label='Train Loss')
plt.plot(metrics_vgg16['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss1, accuracy1= model_vgg16.evaluate(X_test, y_cat_test, verbose=0)
print(f"Test Loss: {loss1}")
print(f"Test Accuracy: {accuracy1}")

base_model1 = VGG19(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# Add custom classification layers
model_vgg19 = Sequential([
    base_model1,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(29, activation='softmax')
])

# Freeze the base model layers
base_model1.trainable = False

model_vgg19.summary()

model_vgg19.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_vgg19 = model_vgg19.fit(
    X_train, y_cat_train,
    epochs=15,
    batch_size=64,
    validation_data=(X_test, y_cat_test),
    callbacks=[early_stop],
    verbose=2
)

metrics_vgg19 = pd.DataFrame(model_vgg19.history.history)
print("The model metrics are")
print(metrics_vgg19)

import matplotlib.pyplot as plt

# Plotting training and validation accuracy
plt.plot(metrics_vgg19['accuracy'], label='Train Accuracy')
plt.plot(metrics_vgg19['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and validation loss
plt.plot(metrics_vgg19['loss'], label='Train Loss')
plt.plot(metrics_vgg19['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss2, accuracy2= model_vgg19.evaluate(X_test, y_cat_test, verbose=0)
print(f"Test Loss: {loss2}")
print(f"Test Accuracy: {accuracy2}")

# Load the pre-trained ResNet50 model without the top classification layers
base_model2 = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# Add custom classification layers
model_resnet50 = Sequential([
    base_model2,
    GlobalAveragePooling2D(),  # Use global average pooling instead of Flatten
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(29, activation='softmax')
])

# Freeze the base model layers
base_model2.trainable = False

model_resnet50.summary()

model_resnet50.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_resnet50 = model_resnet50.fit(
    X_train, y_cat_train,
    epochs=15,
    batch_size=64,
    validation_data=(X_test, y_cat_test),
)

metrics_resnet50 = pd.DataFrame(model_resnet50.history.history)
print("The model metrics are")
print(metrics_resnet50)

import matplotlib.pyplot as plt

# Plotting training and validation accuracy
plt.plot(metrics_resnet50['accuracy'], label='Train Accuracy')
plt.plot(metrics_resnet50['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and validation loss
plt.plot(metrics_resnet50['loss'], label='Train Loss')
plt.plot(metrics_resnet50['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss3, accuracy3= model_resnet50.evaluate(X_test, y_cat_test, verbose=0)
print(f"Test Loss: {loss3}")
print(f"Test Accuracy: {accuracy3}")

# Load the pre-trained MobileNet model without the top classification layers
base_model3 = MobileNet(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# Add custom classification layers
model_mobilenet = Sequential([
    base_model3,
    GlobalAveragePooling2D(),  # Use global average pooling instead of Flatten
    Dense(128, activation='relu'),  # Smaller dense layer since MobileNet is lightweight
    Dropout(0.3),
    Dense(29, activation='softmax')
])

# Freeze the base model layers
base_model3.trainable = False

base_model3.summary()

model_mobilenet.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history_mobilenet = model_mobilenet.fit(
    X_train, y_cat_train,
    epochs=20,
    batch_size=64,
    validation_data=(X_test, y_cat_test),
)

metrics_mobilenet = pd.DataFrame(model_mobilenet.history.history)
print("The model_MobileNet metrics are")
print(metrics_mobilenet)

import matplotlib.pyplot as plt

# Plotting training and validation accuracy
plt.plot(metrics_mobilenet['accuracy'], label='Train Accuracy')
plt.plot(metrics_mobilenet['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting training and validation loss
plt.plot(metrics_mobilenet['loss'], label='Train Loss')
plt.plot(metrics_mobilenet['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss4, accuracy4= model_mobilenet.evaluate(X_test, y_cat_test, verbose=0)
print(f"Test Loss: {loss4}")
print(f"Test Accuracy: {accuracy4}")

# Generate predictions for the test set
predictions = np.argmax(model.predict(X_test), axis=-1)

# Print a confirmation message
print("Done!")

predictions1 = np.argmax(model_vgg16.predict(X_test), axis=-1)

# Print a confirmation message
print("Done!")

predictions2 = np.argmax(model_vgg19.predict(X_test), axis=-1)

# Print a confirmation message
print("Done!")

predictions3 = np.argmax(model_resnet50.predict(X_test), axis=-1)

# Print a confirmation message
print("Done!")

predictions4 = np.argmax(model_mobilenet.predict(X_test), axis=-1)

# Print a confirmation message
print("Done!")

# Visualize a few sample predictions
for i in range(5):  # Display first 5 predictions
    plt.imshow(X_test[i])
    plt.title(f"Predicted: {predictions[i]}, Actual: {y_test[i]}")
    plt.show()

from sklearn.metrics import classification_report, confusion_matrix

# Generate classification report
print("Classification Report:")
print(classification_report(y_test, predictions))

# Generate confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions))

from sklearn.metrics import classification_report, confusion_matrix

# Generate classification report
print("Classification Report1:")
print(classification_report(y_test, predictions1))

# Generate confusion matrix
print("Confusion Matrix1:")
print(confusion_matrix(y_test, predictions1))

from sklearn.metrics import classification_report, confusion_matrix

# Generate classification report
print("Classification Report2:")
print(classification_report(y_test, predictions2))

# Generate confusion matrix
print("Confusion Matrix2:")
print(confusion_matrix(y_test, predictions2))

from sklearn.metrics import classification_report, confusion_matrix

# Generate classification report
print("Classification Report3:")
print(classification_report(y_test, predictions3))

# Generate confusion matrix
print("Confusion Matrix3:")
print(confusion_matrix(y_test, predictions3))

from sklearn.metrics import classification_report, confusion_matrix

# Generate classification report
print("Classification Report4:")
print(classification_report(y_test, predictions4))

# Generate confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions4))

# Generate confusion matrix
cm = confusion_matrix(y_test, predictions)

# Create the heatmap plot
plt.figure(figsize=(12, 12))  # Set figure size
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(29), yticklabels=range(29))

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')

# Display the plot
plt.show()

# Generate confusion matrix
cm = confusion_matrix(y_test, predictions1)

# Create the heatmap plot
plt.figure(figsize=(12, 12))  # Set figure size
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(29), yticklabels=range(29))

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')

# Display the plot
plt.show()

# Generate confusion matrix
cm = confusion_matrix(y_test, predictions2)

# Create the heatmap plot
plt.figure(figsize=(12, 12))  # Set figure size
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(29), yticklabels=range(29))

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')

# Display the plot
plt.show()

# Generate confusion matrix
cm = confusion_matrix(y_test, predictions3)

# Create the heatmap plot
plt.figure(figsize=(12, 12))  # Set figure size
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(29), yticklabels=range(29))

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')

# Display the plot
plt.show()

# Generate confusion matrix
cm = confusion_matrix(y_test, predictions4)

# Create the heatmap plot
plt.figure(figsize=(12, 12))  # Set figure size
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(29), yticklabels=range(29))

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')

# Display the plot
plt.show()

from keras.models import load_model
model.save('ASL_CNN.h5')
print("Model saved successfully...")

model_vgg16.save('vgg16.h5')

model_vgg19.save('vgg19.h5')

model_resnet50.save('resnet50.h5')

model_mobilenet.save('mobilenet.h5')

# Load the pre-trained model
model_path = "path_to_your_saved_model.h5"
model = load_model(model_path)

# Define a function for preprocessing images
def preprocess_image(image_path, target_size=(64, 64)):
    """
    Preprocesses the image for model prediction.
    :param image_path: Path to the image
    :param target_size: Target size for resizing the image
    :return: Preprocessed image
    """
    img = load_img(image_path, target_size=target_size)
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array / 255.0  # Normalize to [0,1]
    return img_array

# Define a function to predict a single image
def predict_single_image(image_path):
    """
    Predicts the class of a single image.
    :param image_path: Path to the image
    :return: Prediction
    """
    preprocessed_image = preprocess_image(image_path)
    prediction = model.predict(preprocessed_image)
    predicted_label = np.argmax(prediction, axis=1)[0]
    return predicted_label

# Allow user to load and predict images one by one
def user_load_and_predict():
    """
    Allows the user to load images one by one and get predictions.
    Outputs the sequence of predicted letters.
    """
    print("Enter the path of the image to predict (or type 'exit' to stop):")
    predicted_sequence = []
    while True:
        image_path = input("Image path: ")
        if image_path.lower() == 'exit':
            print("Exiting prediction loop.")
            break
        if not os.path.exists(image_path):
            print("Invalid path. Please try again.")
            continue
        try:
            prediction = predict_single_image(image_path)
            predicted_sequence.append(prediction)
        except Exception as e:
            print(f"Error processing image: {e}")
    print("Predicted Sequence:", "".join(map(str, predicted_sequence)))

# Example usage
if __name__ == "__main__":
    user_load_and_predict()